{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data\n",
    "df = pd.read_csv('resources/DATA.csv')\n",
    "\n",
    "# Target Columns\n",
    "target_columns = [\n",
    "    'HN',\n",
    "    'HNE',\n",
    "    'HE',\n",
    "    'HSE',\n",
    "    'HS',\n",
    "    'HSW',\n",
    "    'HW',\n",
    "    'HNW',\n",
    "    'MN',\n",
    "    'MNE',\n",
    "    'ME',\n",
    "    'MSE',\n",
    "    'MS',\n",
    "    'MSW',\n",
    "    'MW',\n",
    "    'MNW',\n",
    "    'LN',\n",
    "    'LNE',\n",
    "    'LE',\n",
    "    'LSE',\n",
    "    'LS',\n",
    "    'LSW',\n",
    "    'LW',\n",
    "    'LNW'\n",
    "]\n",
    "\n",
    "# Convert colors to numbers\n",
    "map = {\n",
    "    'gray': 0,\n",
    "    'green': 1,\n",
    "    'yellow': 2,\n",
    "    'orange': 3,\n",
    "    'red': 4,\n",
    "    'black': 5,    \n",
    "}\n",
    "\n",
    "# For target columns\n",
    "for col in target_columns:\n",
    "    df[col] = df[col].map(map)\n",
    "\n",
    "# Make a new column be the max risk of the target columns\n",
    "df['max_risk'] = df[target_columns].max(axis=1)\n",
    "target_columns += ['max_risk']\n",
    "    \n",
    "input_columns = [\n",
    "    'ALTA_Precip_(tenths_mm)',\n",
    "    'ALTA_Snowfall_(mm)',\n",
    "    'ALTA_Snow_Depth_(mm)',\n",
    "    'ALTA_Max_Temp_(tenths_C)',\n",
    "    'ALTA_Min_Temp_(tenths_C)',\n",
    "    'ALTA_Multiday_Prec_Days',\n",
    "    'ALTA_Multiday_Snowfall_Days',\n",
    "    'ALTA_Multiday_Prec_Days_(tenths_mm)',\n",
    "    'ALTA_Multiday_Snowfall',\n",
    "    'ALTA_Temp_at_Observation_(tenths_C)',\n",
    "    'ALTA_Fog_Ice_Fog',\n",
    "    'ALTA_Heavy_Fog_Freezing_Fog',\n",
    "    'ALTA_Ice_Pellets_Sleet',\n",
    "    'ALTA_Hail',\n",
    "    'ALTA_Glaze_Rime',\n",
    "    'ALTA_Weather_Type_07',\n",
    "    'ALTA_Smoke_Haze',\n",
    "    'ALTA_Blowing_Drifting_Snow',\n",
    "    'ALTA_High_Winds',\n",
    "    'ALTA_Snow_Pellets_Ice_Crystals',\n",
    "    'BRIGHTON_Precip_(tenths_mm)',\n",
    "    'BRIGHTON_Snow_Depth_(mm)',\n",
    "    'BRIGHTON_Max_Temp_(tenths_C)',\n",
    "    'BRIGHTON_Min_Temp_(tenths_C)',\n",
    "    'BRIGHTON_Avg_Daily_Temp_(tenths_C)',\n",
    "    'BRIGHTON_Temp_at_Observation_(tenths_C)',\n",
    "    'BRIGHTON_Snow_on_Ground_(tenths_mm)',\n",
    "    'THAYNES_Precip_(tenths_mm)',\n",
    "    'THAYNES_Snow_Depth_(mm)',\n",
    "    'THAYNES_Max_Temp_(tenths_C)',\n",
    "    'THAYNES_Min_Temp_(tenths_C)',\n",
    "    'THAYNES_Avg_Daily_Temp_(tenths_C)',\n",
    "    'THAYNES_Temp_at_Observation_(tenths_C)',\n",
    "    'THAYNES_Snow_on_Ground_(tenths_mm)',\n",
    "    'PARLEY_SUMMIT_Precip_(tenths_mm)',\n",
    "    'PARLEY_SUMMIT_Snow_Depth_(mm)',\n",
    "    'PARLEY_SUMMIT_Max_Temp_(tenths_C)',\n",
    "    'PARLEY_SUMMIT_Min_Temp_(tenths_C)',\n",
    "    'PARLEY_SUMMIT_Temp_at_Observation_(tenths_C)',\n",
    "    'PARLEY_SUMMIT_Snow_on_Ground_(tenths_mm)'\n",
    "]\n",
    "\n",
    "# add data from previous day\n",
    "for col in target_columns:\n",
    "    input_columns.append(col + '_prev')\n",
    "    df[col + '_prev'] = df[col].shift(1)\n",
    "\n",
    "# Fill NA with 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "X = df[input_columns]\n",
    "y = df['max_risk']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:  0.42394822006472493\n",
      "Max:  (100, 100, 100, 100, 100, 100, 100) 0.5307443365695793\n"
     ]
    }
   ],
   "source": [
    "# Just for my own curiosity I'll run a few quick models\n",
    "# MLP, Decision Tree, KNN\n",
    "# MLP Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mlp_accuracies = {}\n",
    "params250 = [(250, 250, 250), (250, 250, 250, 250), (250, 250, 250, 250, 250), (250, 250, 250, 250, 250, 250), (250, 250, 250, 250, 250, 250, 250)]\n",
    "params100 = [(100, 100, 100), (100, 100, 100, 100), (100, 100, 100, 100, 100), (100, 100, 100, 100, 100, 100), (100, 100, 100, 100, 100, 100, 100)]\n",
    "params32 = [(32, 32, 32), (32, 32, 32, 32), (32, 32, 32, 32, 32), (32, 32, 32, 32, 32, 32)]\n",
    "params10 = [(10, 10, 10), (10, 10, 10, 10), (10, 10, 10, 10, 10), (10, 10, 10, 10, 10, 10)]\n",
    "params = params100 + params32 + params10\n",
    "for param in params:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=param, early_stopping=True, max_iter=1000)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    mlp_accuracies[param] = accuracy_score(y_test, y_pred)\n",
    "    results[('MLPClassifer', param)] = {\n",
    "        'model': mlp,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print baseline accuracy\n",
    "print(\"Baseline: \", y_test.value_counts().max() / len(y_test)) \n",
    "\n",
    "# Print MLP accuracies\n",
    "print(\"Max: \", max(mlp_accuracies, key=mlp_accuracies.get), mlp_accuracies[max(mlp_accuracies, key=mlp_accuracies.get)])\n",
    "# for param in params:\n",
    "#     print(param, mlp_accuracies[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  3 0.7022653721682848\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_accuracies = {}\n",
    "params = range(1, 20)\n",
    "for param in params:\n",
    "    dt = DecisionTreeClassifier(max_depth=param)\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    dt_accuracies[param] = accuracy_score(y_test, y_pred)\n",
    "    results[('DecisionTreeClassifier', param)] = {\n",
    "        'model': dt,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Print Decision Tree accuracies\n",
    "print(\"Max: \", max(dt_accuracies, key=dt_accuracies.get), dt_accuracies[max(dt_accuracies, key=dt_accuracies.get)])\n",
    "# for param in params:\n",
    "#     print(param, dt_accuracies[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  1 0.6375404530744336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_accuracies = {}\n",
    "params = range(1, 20)\n",
    "for param in params:\n",
    "    knn = KNeighborsClassifier(n_neighbors=param)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    knn_accuracies[param] = accuracy_score(y_test, y_pred)\n",
    "    results[('KNeighborsClassifier', param)] = {\n",
    "        'model': knn,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print KNN accuracies\n",
    "print(\"Max: \", max(knn_accuracies, key=knn_accuracies.get), knn_accuracies[max(knn_accuracies, key=knn_accuracies.get)])\n",
    "# for param in params:\n",
    "#     print(param, knn_accuracies[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  11 0.7119741100323624\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_accuracies = {}\n",
    "params = range(1, 20)\n",
    "for param in params:\n",
    "    rf = RandomForestClassifier(max_depth=param)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    rf_accuracies[param] = accuracy_score(y_test, y_pred)\n",
    "    results[('RandomForestClassifier', param)] = {\n",
    "        'model': rf,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print Random Forest accuracies\n",
    "print(\"Max: \", max(rf_accuracies, key=rf_accuracies.get), rf_accuracies[max(rf_accuracies, key=rf_accuracies.get)])\n",
    "# for param in params:\n",
    "#     print(param, rf_accuracies[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  5 0.7119741100323624\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gradient Boosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_accuracies = {}\n",
    "params = range(5, 6)\n",
    "for param in params:\n",
    "    gb = GradientBoostingClassifier(max_depth=param)\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred = gb.predict(X_test)\n",
    "    gb_accuracies[param] = accuracy_score(y_test, y_pred)\n",
    "    results[('GradientBoostingClassifier', param)] = {\n",
    "        'model': gb,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print Gradient Boosting accuracies\n",
    "print(\"Max: \", max(gb_accuracies, key=gb_accuracies.get), gb_accuracies[max(gb_accuracies, key=gb_accuracies.get)])\n",
    "# for param in params:\n",
    "#     print(param, gb_accuracies[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  11 0.6796116504854369\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ab_accuracies = {}\n",
    "params = range(1, 20)\n",
    "for param in params:\n",
    "    ab = AdaBoostClassifier(n_estimators=param)\n",
    "    ab.fit(X_train, y_train)\n",
    "    y_pred = ab.predict(X_test)\n",
    "    ab_accuracies[param] = accuracy_score(y_test, y_pred)\n",
    "    results[('AdaBoostClassifier', param)] = {\n",
    "        'model': ab,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print AdaBoost accuracies\n",
    "print(\"Max: \", max(ab_accuracies, key=ab_accuracies.get), ab_accuracies[max(ab_accuracies, key=ab_accuracies.get)])\n",
    "# for param in params:\n",
    "#     print(param, ab_accuracies[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  5 0.7313915857605178\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_accuracies = {}\n",
    "params = range(1, 20)\n",
    "for param in params:\n",
    "    xgb = XGBClassifier(max_depth=param)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    xgb_accuracies[param] = accuracy_score(y_test, y_pred)\n",
    "    results[('XGBClassifier', param)] = {\n",
    "        'model': xgb,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print XGBoost accuracies\n",
    "print(\"Max: \", max(xgb_accuracies, key=xgb_accuracies.get), xgb_accuracies[max(xgb_accuracies, key=xgb_accuracies.get)])\n",
    "# for param in params:\n",
    "#     print(param, xgb_accuracies[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  16 0.6051779935275081\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SVM Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_accuracies = {}\n",
    "params = range(1, 20)\n",
    "for param in params:\n",
    "    svm = SVC(C=param)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    svm_accuracies[param] = accuracy_score(y_test, y_pred)\n",
    "    results[('SVC', param)] = {\n",
    "        'model': svm,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print SVM accuracies\n",
    "print(\"Max: \", max(svm_accuracies, key=svm_accuracies.get), svm_accuracies[max(svm_accuracies, key=svm_accuracies.get)])\n",
    "# for param in params:\n",
    "#     print(param, svm_accuracies[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  sag 0.6731391585760518\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_accuracies = {}\n",
    "#‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’\n",
    "params = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "\n",
    "# preprocess X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "pX_train = scaler.fit_transform(X_train)\n",
    "pX_test = scaler.transform(X_test)\n",
    "\n",
    "for param in params:\n",
    "    lr = LogisticRegression(solver=param, max_iter=10000)\n",
    "    lr.fit(pX_train, y_train)\n",
    "    y_pred = lr.predict(pX_test)\n",
    "    lr_accuracies[param] = accuracy_score(y_test, y_pred)\n",
    "    results[('LogisticRegression', param)] = {\n",
    "        'model': lr,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print Logistic Regression accuracies\n",
    "print(\"Max: \", max(lr_accuracies, key=lr_accuracies.get), lr_accuracies[max(lr_accuracies, key=lr_accuracies.get)])\n",
    "# for param in params:\n",
    "#     print(param, lr_accuracies[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5598705501618123\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gaussian Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "gnb_accuracy = accuracy_score(y_test, y_pred)\n",
    "results[('GaussianNB', '')] = {\n",
    "    'model': gnb,\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'mse': mean_squared_error(y_test, y_pred)\n",
    "}\n",
    "    \n",
    "# Print Gaussian Naive Bayes accuracies\n",
    "print(gnb_accuracy)\n",
    "# for param in params:\n",
    "#     print(param, gnb_accuracies[param])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  (100, 100, 100, 100, 100, 100, 100) 0.5247558660274091\n",
      "Max:  (100, 100, 100, 100, 100, 100, 100) 0.5372168284789643\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regressors\n",
    "# MLP Regressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mlpr_mse = {}\n",
    "mlpr_accuracy = {}\n",
    "params250 = [(250, 250, 250), (250, 250, 250, 250), (250, 250, 250, 250, 250), (250, 250, 250, 250, 250, 250), (250, 250, 250, 250, 250, 250, 250)]\n",
    "params100 = [(100, 100, 100), (100, 100, 100, 100), (100, 100, 100, 100, 100), (100, 100, 100, 100, 100, 100, 100)]\n",
    "params32 = [(32, 32, 32), (32, 32, 32, 32), (32, 32, 32, 32, 32), (32, 32, 32, 32, 32, 32)]\n",
    "params10 = [(10, 10, 10), (10, 10, 10, 10), (10, 10, 10, 10, 10), (10, 10, 10, 10, 10, 10)]\n",
    "params = params100 + params32 + params10\n",
    "for param in params:\n",
    "    mlpr = MLPRegressor(hidden_layer_sizes=param, early_stopping=True, max_iter=1000)\n",
    "    mlpr.fit(X_train, y_train)\n",
    "    y_pred = mlpr.predict(X_test)\n",
    "    mlpr_mse[param] = mean_squared_error(y_test, y_pred)\n",
    "    mlpr_accuracy[param] = accuracy_score(y_test, y_pred.round())\n",
    "    results[('MLPRegressor', param)] = {\n",
    "        'model': mlpr,\n",
    "        'accuracy': accuracy_score(y_test, y_pred.round()),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Print MLP Regressor accuracies\n",
    "print(\"Min: \", min(mlpr_mse, key=mlpr_mse.get), mlpr_mse[min(mlpr_mse, key=mlpr_mse.get)])\n",
    "print(\"Max: \", max(mlpr_accuracy, key=mlpr_accuracy.get), mlpr_accuracy[max(mlpr_accuracy, key=mlpr_accuracy.get)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  5 0.32694036677592286\n",
      "Max:  6 0.6990291262135923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Decision Tree Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr_mse = {}\n",
    "dtr_accuracy = {}\n",
    "params = range(1, 20)\n",
    "for param in params:\n",
    "    dtr = DecisionTreeRegressor(max_depth=param)\n",
    "    dtr.fit(X_train, y_train)\n",
    "    y_pred = dtr.predict(X_test)\n",
    "    dtr_mse[param] = mean_squared_error(y_test, y_pred)\n",
    "    dtr_accuracy[param] = accuracy_score(y_test, y_pred.round())\n",
    "    results[('DecisionTreeRegressor', param)] = {\n",
    "        'model': dtr,\n",
    "        'accuracy': accuracy_score(y_test, y_pred.round()),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print Decision Tree Regressor accuracies\n",
    "print(\"Min: \", min(dtr_mse, key=dtr_mse.get), dtr_mse[min(dtr_mse, key=dtr_mse.get)])\n",
    "print(\"Max: \", max(dtr_accuracy, key=dtr_accuracy.get), dtr_accuracy[max(dtr_accuracy, key=dtr_accuracy.get)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  (2, 'chebyshev') 0.4199029126213592\n",
      "Max:  (1, 'manhattan') 0.6504854368932039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# KNN Regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knr_mse = {}\n",
    "knr_accuracy = {}\n",
    "params = range(1, 20)\n",
    "params2 = ['euclidean', 'manhattan', 'chebyshev']\n",
    "\n",
    "for param in params:\n",
    "    for param2 in params2:\n",
    "        knr = KNeighborsRegressor(n_neighbors=param, metric=param2)\n",
    "        knr.fit(X_train, y_train)\n",
    "        y_pred = knr.predict(X_test)\n",
    "        knr_mse[(param, param2)] = mean_squared_error(y_test, y_pred)\n",
    "        knr_accuracy[(param, param2)] = accuracy_score(y_test, y_pred.round())\n",
    "        results[('KNeighborsRegressor', (param, param2))] = {\n",
    "            'model': knr,\n",
    "            'accuracy': accuracy_score(y_test, y_pred.round()),\n",
    "            'mse': mean_squared_error(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "# Print KNN Regressor accuracies\n",
    "print(\"Min: \", min(knr_mse, key=knr_mse.get), knr_mse[min(knr_mse, key=knr_mse.get)])\n",
    "print(\"Max: \", max(knr_accuracy, key=knr_accuracy.get), knr_accuracy[max(knr_accuracy, key=knr_accuracy.get)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  16 0.26609003854388097\n",
      "Max:  12 0.7184466019417476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr_mse = {}\n",
    "rfr_accuracy = {}\n",
    "params = range(1, 20)\n",
    "for param in params:\n",
    "    rfr = RandomForestRegressor(max_depth=param)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    y_pred = rfr.predict(X_test)\n",
    "    rfr_mse[param] = mean_squared_error(y_test, y_pred)\n",
    "    rfr_accuracy[param] = accuracy_score(y_test, y_pred.round())\n",
    "    results[('RandomForestRegressor', param)] = {\n",
    "        'model': rfr,\n",
    "        'accuracy': accuracy_score(y_test, y_pred.round()),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print Random Forest Regressor accuracies\n",
    "print(\"Min: \", min(rfr_mse, key=rfr_mse.get), rfr_mse[min(rfr_mse, key=rfr_mse.get)])\n",
    "print(\"Max: \", max(rfr_accuracy, key=rfr_accuracy.get), rfr_accuracy[max(rfr_accuracy, key=rfr_accuracy.get)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  2 0.2643110258098708\n",
      "Max:  3 0.6990291262135923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# XGBoost Regressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgbr_mse = {}\n",
    "xgbr_accuracy = {}\n",
    "params = range(1, 20)\n",
    "for param in params:\n",
    "    xgbr = XGBRegressor(max_depth=param)\n",
    "    xgbr.fit(X_train, y_train)\n",
    "    y_pred = xgbr.predict(X_test)\n",
    "    xgbr_mse[param] = mean_squared_error(y_test, y_pred)\n",
    "    xgbr_accuracy[param] = accuracy_score(y_test, y_pred.round())\n",
    "    results[('XGBRegressor', param)] = {\n",
    "        'model': xgbr,\n",
    "        'accuracy': accuracy_score(y_test, y_pred.round()),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print XGBoost Regressor accuracies\n",
    "print(\"Min: \", min(xgbr_mse, key=xgbr_mse.get), xgbr_mse[min(xgbr_mse, key=xgbr_mse.get)])\n",
    "print(\"Max: \", max(xgbr_accuracy, key=xgbr_accuracy.get), xgbr_accuracy[max(xgbr_accuracy, key=xgbr_accuracy.get)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  19 0.4861573664278051\n",
      "Max:  18 0.6019417475728155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SVM Regressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_mse = {}\n",
    "svr_accuracy = {}\n",
    "params = range(1, 20)\n",
    "\n",
    "for param in params:\n",
    "    svr = SVR(C=param)\n",
    "    svr.fit(X_train, y_train)\n",
    "    y_pred = svr.predict(X_test)\n",
    "    svr_mse[param] = mean_squared_error(y_test, y_pred)\n",
    "    svr_accuracy[param] = accuracy_score(y_test, y_pred.round())\n",
    "    results[('SVR', param)] = {\n",
    "        'model': svr,\n",
    "        'accuracy': accuracy_score(y_test, y_pred.round()),\n",
    "        'mse': mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "# Print SVM Regressor accuracies\n",
    "print(\"Min: \", min(svr_mse, key=svr_mse.get), svr_mse[min(svr_mse, key=svr_mse.get)])\n",
    "print(\"Max: \", max(svr_accuracy, key=svr_accuracy.get), svr_accuracy[max(svr_accuracy, key=svr_accuracy.get)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  MLP Classifier\n",
      "Param:  (250, 250, 250)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLPClassifier.__init__() got an unexpected keyword argument 'n_neighbors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Data\\BYU\\Fall2023\\201R\\avalanche\\starting_code.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Data/BYU/Fall2023/201R/avalanche/starting_code.ipynb#X40sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     pX_test \u001b[39m=\u001b[39m X_test\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Data/BYU/Fall2023/201R/avalanche/starting_code.ipynb#X40sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39mif\u001b[39;00m other_params[model] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Data/BYU/Fall2023/201R/avalanche/starting_code.ipynb#X40sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     m \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_params[model], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mn_neighbors\u001b[39;49m\u001b[39m'\u001b[39;49m: param})\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Data/BYU/Fall2023/201R/avalanche/starting_code.ipynb#X40sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Data/BYU/Fall2023/201R/avalanche/starting_code.ipynb#X40sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m     m \u001b[39m=\u001b[39m model(param)\n",
      "\u001b[1;31mTypeError\u001b[0m: MLPClassifier.__init__() got an unexpected keyword argument 'n_neighbors'"
     ]
    }
   ],
   "source": [
    "# DO ALL ABOVE, BUT ALGORITHMICALLY\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "models = [\n",
    "    MLPClassifier,\n",
    "    DecisionTreeClassifier,\n",
    "    KNeighborsClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    XGBClassifier,\n",
    "    SVC,\n",
    "    LogisticRegression,\n",
    "    LinearRegression,\n",
    "    GaussianNB,\n",
    "    MLPRegressor,\n",
    "    DecisionTreeRegressor,\n",
    "    KNeighborsRegressor,\n",
    "    RandomForestRegressor,\n",
    "    XGBRegressor,\n",
    "    SVR\n",
    "]\n",
    "\n",
    "model_accuracies = {}\n",
    "model_mses = {}\n",
    "\n",
    "# give parameter options for each model\n",
    "params = {\n",
    "    MLPClassifier: [(250, 250, 250), (250, 250, 250, 250), (250, 250, 250, 250, 250), (250, 250, 250, 250, 250, 250), (250, 250, 250, 250, 250, 250, 250),\n",
    "                    (100, 100, 100), (100, 100, 100, 100), (100, 100, 100, 100, 100), (100, 100, 100, 100, 100, 100, 100),\n",
    "                    (32, 32, 32), (32, 32, 32, 32), (32, 32, 32, 32, 32), (32, 32, 32, 32, 32, 32),\n",
    "                    (10, 10, 10), (10, 10, 10, 10), (10, 10, 10, 10, 10)],\n",
    "    DecisionTreeClassifier: range(1, 20),\n",
    "    KNeighborsClassifier: range(1, 20),\n",
    "    RandomForestClassifier: range(1, 20),\n",
    "    GradientBoostingClassifier: range(1, 20),\n",
    "    AdaBoostClassifier: range(1, 20),\n",
    "    XGBClassifier: range(1, 20),\n",
    "    SVC: range(1, 20),\n",
    "    LogisticRegression: ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    LinearRegression: None,\n",
    "    GaussianNB: None,\n",
    "    MLPRegressor: [(250, 250, 250), (250, 250, 250, 250), (250, 250, 250, 250, 250), (250, 250, 250, 250, 250, 250, 250),\n",
    "                    (100, 100, 100), (100, 100, 100, 100), (100, 100, 100, 100, 100), (100, 100, 100, 100, 100, 100, 100),\n",
    "                    (32, 32, 32), (32, 32, 32, 32), (32, 32, 32, 32, 32), (32, 32, 32, 32, 32, 32),\n",
    "                    (10, 10, 10), (10, 10, 10, 10), (10, 10, 10, 10, 10)],\n",
    "    DecisionTreeRegressor: range(1, 20),\n",
    "    KNeighborsRegressor: range(1, 20),\n",
    "    RandomForestRegressor: range(1, 20),\n",
    "    XGBRegressor: range(1, 20),\n",
    "    SVR: range(1, 20)\n",
    "}\n",
    "\n",
    "other_params = {\n",
    "    MLPClassifier: {'early_stopping': True, 'max_iter': 1000},\n",
    "    DecisionTreeClassifier: {},\n",
    "    KNeighborsClassifier: {},\n",
    "    RandomForestClassifier: {},\n",
    "    GradientBoostingClassifier: {},\n",
    "    AdaBoostClassifier: {},\n",
    "    XGBClassifier: {},\n",
    "    SVC: {},\n",
    "    LogisticRegression: {'max_iter': 10000},\n",
    "    LinearRegression: {},\n",
    "    GaussianNB: {},\n",
    "    MLPRegressor: {'early_stopping': True, 'max_iter': 1000},\n",
    "    DecisionTreeRegressor: {},\n",
    "    KNeighborsRegressor: {},\n",
    "    RandomForestRegressor: {},\n",
    "    XGBRegressor: {},\n",
    "    SVR: {}\n",
    "}\n",
    "    \n",
    "\n",
    "model_preprocess = {\n",
    "    MLPClassifier: None,\n",
    "    DecisionTreeClassifier: None,\n",
    "    KNeighborsClassifier: None,\n",
    "    RandomForestClassifier: None,\n",
    "    GradientBoostingClassifier: None,\n",
    "    AdaBoostClassifier: None,\n",
    "    XGBClassifier: None,\n",
    "    SVC: None,\n",
    "    LogisticRegression: StandardScaler(),\n",
    "    LinearRegression: StandardScaler(),\n",
    "    GaussianNB: None,\n",
    "    MLPRegressor: StandardScaler(),\n",
    "    DecisionTreeRegressor: None,\n",
    "    KNeighborsRegressor: StandardScaler(),\n",
    "    RandomForestRegressor: None,\n",
    "    XGBRegressor: None,\n",
    "    SVR: None\n",
    "}\n",
    "\n",
    "model_names = {\n",
    "    MLPClassifier: 'MLP Classifier',\n",
    "    DecisionTreeClassifier: 'Decision Tree Classifier',\n",
    "    KNeighborsClassifier: 'K Neighbors Classifier',\n",
    "    RandomForestClassifier: 'Random Forest Classifier',\n",
    "    GradientBoostingClassifier: 'Gradient Boosting Classifier',\n",
    "    AdaBoostClassifier: 'Ada Boost Classifier',\n",
    "    XGBClassifier: 'XGBoost Classifier',\n",
    "    SVC: 'SVM Classifier',\n",
    "    LogisticRegression: 'Logistic Regression Classifier',\n",
    "    LinearRegression: 'Linear Regression',\n",
    "    GaussianNB: 'Gaussian Naive Bayes Classifier',\n",
    "    MLPRegressor: 'MLP Regressor',\n",
    "    DecisionTreeRegressor: 'Decision Tree Regressor',\n",
    "    KNeighborsRegressor: 'K Neighbors Regressor',\n",
    "    RandomForestRegressor: 'Random Forest Regressor',\n",
    "    XGBRegressor: 'XGBoost Regressor',\n",
    "    SVR: 'SVM Regressor'\n",
    "}\n",
    "\n",
    "for model in models:\n",
    "    print(\"Model: \", model_names[model])\n",
    "    model_accuracies[model] = {}\n",
    "    model_mses[model] = {}\n",
    "    for param in params[model]:\n",
    "        print(\"Param: \", param)\n",
    "        if model_preprocess[model] is not None:\n",
    "            pX_train = model_preprocess[model].fit_transform(X_train)\n",
    "            pX_test = model_preprocess[model].transform(X_test)\n",
    "        else:\n",
    "            pX_train = X_train\n",
    "            pX_test = X_test\n",
    "        if other_params[model] is not None:\n",
    "            m = model(**other_params[model], **{'n_neighbors': param})\n",
    "        else:\n",
    "            m = model(param)\n",
    "        m.fit(pX_train, y_train)\n",
    "        y_pred = m.predict(pX_test)\n",
    "        if model in [LinearRegression, GaussianNB]:\n",
    "            model_mses[model][param] = mean_squared_error(y_test, y_pred)\n",
    "        else:\n",
    "            model_accuracies[model][param] = accuracy_score(y_test, y_pred.round())\n",
    "            model_mses[model][param] = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Max Accuracy: \", max(model_accuracies[model], key=model_accuracies[model].get), model_accuracies[model][max(model_accuracies[model], key=model_accuracies[model].get)])\n",
    "    print(\"Min MSE: \", min(model_mses[model], key=model_mses[model].get), model_mses[model][min(model_mses[model], key=model_mses[model].get)])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MLPClassifer', (100, 100, 100)) 0.44660194174757284 1.1423948220064726\n",
      "('MLPClassifer', (100, 100, 100, 100)) 0.47572815533980584 0.8478964401294499\n",
      "('MLPClassifer', (100, 100, 100, 100, 100)) 0.5145631067961165 1.0550161812297734\n",
      "('MLPClassifer', (100, 100, 100, 100, 100, 100)) 0.5048543689320388 0.8802588996763754\n",
      "('MLPClassifer', (100, 100, 100, 100, 100, 100, 100)) 0.5307443365695793 1.0323624595469256\n",
      "('MLPClassifer', (32, 32, 32)) 0.42718446601941745 1.1585760517799353\n",
      "('MLPClassifer', (32, 32, 32, 32)) 0.45307443365695793 1.2944983818770226\n",
      "('MLPClassifer', (32, 32, 32, 32, 32)) 0.42071197411003236 1.0906148867313916\n",
      "('MLPClassifer', (32, 32, 32, 32, 32, 32)) 0.34951456310679613 1.233009708737864\n",
      "('MLPClassifer', (10, 10, 10)) 0.4627831715210356 1.692556634304207\n",
      "('MLPClassifer', (10, 10, 10, 10)) 0.43042071197411 1.6213592233009708\n",
      "('MLPClassifer', (10, 10, 10, 10, 10)) 0.4368932038834951 1.27831715210356\n",
      "('MLPClassifer', (10, 10, 10, 10, 10, 10)) 0.4368932038834951 1.0453074433656957\n",
      "('DecisionTreeClassifier', 1) 0.5533980582524272 0.6796116504854369\n",
      "('DecisionTreeClassifier', 2) 0.6181229773462783 0.459546925566343\n",
      "('DecisionTreeClassifier', 3) 0.7022653721682848 0.37540453074433655\n",
      "('DecisionTreeClassifier', 4) 0.6957928802588996 0.35275080906148865\n",
      "('DecisionTreeClassifier', 5) 0.6893203883495146 0.3592233009708738\n",
      "('DecisionTreeClassifier', 6) 0.6731391585760518 0.3851132686084142\n",
      "('DecisionTreeClassifier', 7) 0.6796116504854369 0.39805825242718446\n",
      "('DecisionTreeClassifier', 8) 0.6731391585760518 0.4401294498381877\n",
      "('DecisionTreeClassifier', 9) 0.6666666666666666 0.43042071197411\n",
      "('DecisionTreeClassifier', 10) 0.6634304207119741 0.4045307443365696\n",
      "('DecisionTreeClassifier', 11) 0.6666666666666666 0.40129449838187703\n",
      "('DecisionTreeClassifier', 12) 0.627831715210356 0.4692556634304207\n",
      "('DecisionTreeClassifier', 13) 0.6504854368932039 0.4563106796116505\n",
      "('DecisionTreeClassifier', 14) 0.6051779935275081 0.5016181229773463\n",
      "('DecisionTreeClassifier', 15) 0.6213592233009708 0.49514563106796117\n",
      "('DecisionTreeClassifier', 16) 0.6181229773462783 0.49838187702265374\n",
      "('DecisionTreeClassifier', 17) 0.6181229773462783 0.5145631067961165\n",
      "('DecisionTreeClassifier', 18) 0.6116504854368932 0.5145631067961165\n",
      "('DecisionTreeClassifier', 19) 0.6213592233009708 0.49514563106796117\n",
      "('KNeighborsClassifier', 1) 0.6375404530744336 0.627831715210356\n",
      "('KNeighborsClassifier', 2) 0.5825242718446602 0.6310679611650486\n",
      "('KNeighborsClassifier', 3) 0.5922330097087378 0.6731391585760518\n",
      "('KNeighborsClassifier', 4) 0.5631067961165048 0.6019417475728155\n",
      "('KNeighborsClassifier', 5) 0.56957928802589 0.6148867313915858\n",
      "('KNeighborsClassifier', 6) 0.5598705501618123 0.7281553398058253\n",
      "('KNeighborsClassifier', 7) 0.5728155339805825 0.6925566343042071\n",
      "('KNeighborsClassifier', 8) 0.5598705501618123 0.6472491909385113\n",
      "('KNeighborsClassifier', 9) 0.5825242718446602 0.6731391585760518\n",
      "('KNeighborsClassifier', 10) 0.5533980582524272 0.7508090614886731\n",
      "('KNeighborsClassifier', 11) 0.5792880258899676 0.7766990291262136\n",
      "('KNeighborsClassifier', 12) 0.5469255663430421 0.7961165048543689\n",
      "('KNeighborsClassifier', 13) 0.5598705501618123 0.7831715210355987\n",
      "('KNeighborsClassifier', 14) 0.56957928802589 0.7864077669902912\n",
      "('KNeighborsClassifier', 15) 0.5533980582524272 0.8187702265372169\n",
      "('KNeighborsClassifier', 16) 0.5469255663430421 0.8252427184466019\n",
      "('KNeighborsClassifier', 17) 0.5598705501618123 0.8220064724919094\n",
      "('KNeighborsClassifier', 18) 0.5533980582524272 0.8220064724919094\n",
      "('KNeighborsClassifier', 19) 0.5436893203883495 0.8576051779935275\n",
      "('RandomForestClassifier', 1) 0.5566343042071198 0.6763754045307443\n",
      "('RandomForestClassifier', 2) 0.6181229773462783 0.43042071197411\n",
      "('RandomForestClassifier', 3) 0.686084142394822 0.40129449838187703\n",
      "('RandomForestClassifier', 4) 0.6925566343042071 0.3851132686084142\n",
      "('RandomForestClassifier', 5) 0.6957928802588996 0.36245954692556637\n",
      "('RandomForestClassifier', 6) 0.7022653721682848 0.34627831715210355\n",
      "('RandomForestClassifier', 7) 0.6957928802588996 0.343042071197411\n",
      "('RandomForestClassifier', 8) 0.7055016181229773 0.343042071197411\n",
      "('RandomForestClassifier', 9) 0.7022653721682848 0.3365695792880259\n",
      "('RandomForestClassifier', 10) 0.7022653721682848 0.3365695792880259\n",
      "('RandomForestClassifier', 11) 0.7119741100323624 0.3268608414239482\n",
      "('RandomForestClassifier', 12) 0.6990291262135923 0.33980582524271846\n",
      "('RandomForestClassifier', 13) 0.7087378640776699 0.3300970873786408\n",
      "('RandomForestClassifier', 14) 0.7022653721682848 0.34627831715210355\n",
      "('RandomForestClassifier', 15) 0.6957928802588996 0.343042071197411\n",
      "('RandomForestClassifier', 16) 0.7119741100323624 0.3268608414239482\n",
      "('RandomForestClassifier', 17) 0.6990291262135923 0.33980582524271846\n",
      "('RandomForestClassifier', 18) 0.6925566343042071 0.34627831715210355\n",
      "('RandomForestClassifier', 19) 0.6957928802588996 0.35275080906148865\n",
      "('GradientBoostingClassifier', 5) 0.7119741100323624 0.3365695792880259\n",
      "('AdaBoostClassifier', 1) 0.5501618122977346 0.6828478964401294\n",
      "('AdaBoostClassifier', 2) 0.6181229773462783 0.42071197411003236\n",
      "('AdaBoostClassifier', 3) 0.6181229773462783 0.42071197411003236\n",
      "('AdaBoostClassifier', 4) 0.6116504854368932 0.4854368932038835\n",
      "('AdaBoostClassifier', 5) 0.6116504854368932 0.4854368932038835\n",
      "('AdaBoostClassifier', 6) 0.6343042071197411 0.4045307443365696\n",
      "('AdaBoostClassifier', 7) 0.6310679611650486 0.4077669902912621\n",
      "('AdaBoostClassifier', 8) 0.6084142394822006 0.4886731391585761\n",
      "('AdaBoostClassifier', 9) 0.6116504854368932 0.4854368932038835\n",
      "('AdaBoostClassifier', 10) 0.6763754045307443 0.37216828478964403\n",
      "('AdaBoostClassifier', 11) 0.6796116504854369 0.36893203883495146\n",
      "('AdaBoostClassifier', 12) 0.6472491909385113 0.42718446601941745\n",
      "('AdaBoostClassifier', 13) 0.6472491909385113 0.42718446601941745\n",
      "('AdaBoostClassifier', 14) 0.6699029126213593 0.4045307443365696\n",
      "('AdaBoostClassifier', 15) 0.6666666666666666 0.4077669902912621\n",
      "('AdaBoostClassifier', 16) 0.6343042071197411 0.43042071197411\n",
      "('AdaBoostClassifier', 17) 0.6310679611650486 0.4336569579288026\n",
      "('AdaBoostClassifier', 18) 0.6666666666666666 0.4077669902912621\n",
      "('AdaBoostClassifier', 19) 0.6666666666666666 0.4077669902912621\n",
      "('XGBClassifier', 1) 0.6925566343042071 0.3656957928802589\n",
      "('XGBClassifier', 2) 0.7087378640776699 0.34951456310679613\n",
      "('XGBClassifier', 3) 0.7087378640776699 0.34951456310679613\n",
      "('XGBClassifier', 4) 0.7152103559870551 0.3333333333333333\n",
      "('XGBClassifier', 5) 0.7313915857605178 0.3268608414239482\n",
      "('XGBClassifier', 6) 0.7152103559870551 0.32362459546925565\n",
      "('XGBClassifier', 7) 0.7119741100323624 0.3365695792880259\n",
      "('XGBClassifier', 8) 0.7281553398058253 0.32038834951456313\n",
      "('XGBClassifier', 9) 0.7119741100323624 0.34627831715210355\n",
      "('XGBClassifier', 10) 0.7152103559870551 0.3333333333333333\n",
      "('XGBClassifier', 11) 0.6990291262135923 0.36893203883495146\n",
      "('XGBClassifier', 12) 0.7152103559870551 0.32362459546925565\n",
      "('XGBClassifier', 13) 0.7184466019417476 0.32038834951456313\n",
      "('XGBClassifier', 14) 0.7055016181229773 0.343042071197411\n",
      "('XGBClassifier', 15) 0.6990291262135923 0.34951456310679613\n",
      "('XGBClassifier', 16) 0.7152103559870551 0.3333333333333333\n",
      "('XGBClassifier', 17) 0.7119741100323624 0.3365695792880259\n",
      "('XGBClassifier', 18) 0.7022653721682848 0.34627831715210355\n",
      "('XGBClassifier', 19) 0.7087378640776699 0.33980582524271846\n",
      "('SVC', 1) 0.47249190938511326 0.9029126213592233\n",
      "('SVC', 2) 0.511326860841424 0.8543689320388349\n",
      "('SVC', 3) 0.540453074433657 0.7411003236245954\n",
      "('SVC', 4) 0.5469255663430421 0.7249190938511327\n",
      "('SVC', 5) 0.5533980582524272 0.7087378640776699\n",
      "('SVC', 6) 0.5566343042071198 0.7055016181229773\n",
      "('SVC', 7) 0.5663430420711975 0.7055016181229773\n",
      "('SVC', 8) 0.5663430420711975 0.7055016181229773\n",
      "('SVC', 9) 0.5857605177993528 0.686084142394822\n",
      "('SVC', 10) 0.5889967637540453 0.6828478964401294\n",
      "('SVC', 11) 0.5889967637540453 0.6925566343042071\n",
      "('SVC', 12) 0.5954692556634305 0.686084142394822\n",
      "('SVC', 13) 0.6019417475728155 0.6504854368932039\n",
      "('SVC', 14) 0.6019417475728155 0.6504854368932039\n",
      "('SVC', 15) 0.6019417475728155 0.6504854368932039\n",
      "('SVC', 16) 0.6051779935275081 0.6375404530744336\n",
      "('SVC', 17) 0.6051779935275081 0.6375404530744336\n",
      "('SVC', 18) 0.6051779935275081 0.6116504854368932\n",
      "('SVC', 19) 0.6051779935275081 0.6116504854368932\n",
      "('LogisticRegression', 'lbfgs') 0.6699029126213593 0.3786407766990291\n",
      "('LogisticRegression', 'liblinear') 0.6666666666666666 0.42718446601941745\n",
      "('LogisticRegression', 'newton-cg') 0.6699029126213593 0.3786407766990291\n",
      "('LogisticRegression', 'newton-cholesky') 0.6634304207119741 0.3948220064724919\n",
      "('LogisticRegression', 'sag') 0.6731391585760518 0.37540453074433655\n",
      "('LogisticRegression', 'saga') 0.6731391585760518 0.37540453074433655\n",
      "('GaussianNB', '') 0.5598705501618123 0.5372168284789643\n",
      "('MLPRegressor', (100, 100, 100)) 0.23624595469255663 5.696101242086121\n",
      "('MLPRegressor', (100, 100, 100, 100)) 0.3559870550161812 1.831778397453901\n",
      "('MLPRegressor', (100, 100, 100, 100, 100)) 0.3268608414239482 1.2652625283255945\n",
      "('MLPRegressor', (100, 100, 100, 100, 100, 100, 100)) 0.5372168284789643 0.5247558660274091\n",
      "('MLPRegressor', (32, 32, 32)) 0.36245954692556637 2.3462110236249325\n",
      "('MLPRegressor', (32, 32, 32, 32)) 0.34627831715210355 2.157237943627018\n",
      "('MLPRegressor', (32, 32, 32, 32, 32)) 0.3074433656957929 2.2265587595758145\n",
      "('MLPRegressor', (32, 32, 32, 32, 32, 32)) 0.4110032362459547 1.3661084739501257\n",
      "('MLPRegressor', (10, 10, 10)) 0.40129449838187703 0.9983266630295629\n",
      "('MLPRegressor', (10, 10, 10, 10)) 0.3656957928802589 0.9027756708718039\n",
      "('MLPRegressor', (10, 10, 10, 10, 10)) 0.42718446601941745 0.9473984750683022\n",
      "('MLPRegressor', (10, 10, 10, 10, 10, 10)) 0.40129449838187703 1.4605087283817348\n",
      "('DecisionTreeRegressor', 1) 0.5566343042071198 0.6133080413562351\n",
      "('DecisionTreeRegressor', 2) 0.6213592233009708 0.3844424545395628\n",
      "('DecisionTreeRegressor', 3) 0.6699029126213593 0.3443550668238862\n",
      "('DecisionTreeRegressor', 4) 0.6601941747572816 0.3298429713254911\n",
      "('DecisionTreeRegressor', 5) 0.6796116504854369 0.32694036677592286\n",
      "('DecisionTreeRegressor', 6) 0.6990291262135923 0.3349182436387196\n",
      "('DecisionTreeRegressor', 7) 0.6828478964401294 0.3768711282598398\n",
      "('DecisionTreeRegressor', 8) 0.6763754045307443 0.3996987390923441\n",
      "('DecisionTreeRegressor', 9) 0.6763754045307443 0.4169810947690067\n",
      "('DecisionTreeRegressor', 10) 0.6893203883495146 0.42194918469797643\n",
      "('DecisionTreeRegressor', 11) 0.6925566343042071 0.42625815168201314\n",
      "('DecisionTreeRegressor', 12) 0.6957928802588996 0.43921997731240875\n",
      "('DecisionTreeRegressor', 13) 0.6731391585760518 0.44145381721655197\n",
      "('DecisionTreeRegressor', 14) 0.6796116504854369 0.42812083369327836\n",
      "('DecisionTreeRegressor', 15) 0.6601941747572816 0.486244308007906\n",
      "('DecisionTreeRegressor', 16) 0.6634304207119741 0.4627231624594925\n",
      "('DecisionTreeRegressor', 17) 0.6601941747572816 0.5066734301408813\n",
      "('DecisionTreeRegressor', 18) 0.6504854368932039 0.4974139693677444\n",
      "('DecisionTreeRegressor', 19) 0.6407766990291263 0.522057751786106\n",
      "('KNeighborsRegressor', (1, 'euclidean')) 0.6375404530744336 0.627831715210356\n",
      "('KNeighborsRegressor', (1, 'manhattan')) 0.6504854368932039 0.5501618122977346\n",
      "('KNeighborsRegressor', (1, 'chebyshev')) 0.6375404530744336 0.49514563106796117\n",
      "('KNeighborsRegressor', (2, 'euclidean')) 0.5954692556634305 0.4401294498381877\n",
      "('KNeighborsRegressor', (2, 'manhattan')) 0.5889967637540453 0.4482200647249191\n",
      "('KNeighborsRegressor', (2, 'chebyshev')) 0.5792880258899676 0.4199029126213592\n",
      "('KNeighborsRegressor', (3, 'euclidean')) 0.56957928802589 0.46422150305645454\n",
      "('KNeighborsRegressor', (3, 'manhattan')) 0.5533980582524272 0.4581085940309242\n",
      "('KNeighborsRegressor', (3, 'chebyshev')) 0.5889967637540453 0.42970154620640055\n",
      "('KNeighborsRegressor', (4, 'euclidean')) 0.5760517799352751 0.44902912621359226\n",
      "('KNeighborsRegressor', (4, 'manhattan')) 0.5533980582524272 0.4678398058252427\n",
      "('KNeighborsRegressor', (4, 'chebyshev')) 0.56957928802589 0.4338592233009709\n",
      "('KNeighborsRegressor', (5, 'euclidean')) 0.5566343042071198 0.4626537216828479\n",
      "('KNeighborsRegressor', (5, 'manhattan')) 0.5598705501618123 0.4732686084142395\n",
      "('KNeighborsRegressor', (5, 'chebyshev')) 0.5501618122977346 0.47067961165048544\n",
      "('KNeighborsRegressor', (6, 'euclidean')) 0.5631067961165048 0.4795936713412442\n",
      "('KNeighborsRegressor', (6, 'manhattan')) 0.5566343042071198 0.46871628910463864\n",
      "('KNeighborsRegressor', (6, 'chebyshev')) 0.5339805825242718 0.49262855088097807\n",
      "('KNeighborsRegressor', (7, 'euclidean')) 0.5663430420711975 0.4919093851132686\n",
      "('KNeighborsRegressor', (7, 'manhattan')) 0.5760517799352751 0.46846311340070007\n",
      "('KNeighborsRegressor', (7, 'chebyshev')) 0.5339805825242718 0.4902582392180173\n",
      "('KNeighborsRegressor', (8, 'euclidean')) 0.5307443365695793 0.5072309870550162\n",
      "('KNeighborsRegressor', (8, 'manhattan')) 0.5598705501618123 0.47360436893203883\n",
      "('KNeighborsRegressor', (8, 'chebyshev')) 0.5275080906148867 0.5029834142394822\n",
      "('KNeighborsRegressor', (9, 'euclidean')) 0.5469255663430421 0.5136841264133606\n",
      "('KNeighborsRegressor', (9, 'manhattan')) 0.5469255663430421 0.48287985936313876\n",
      "('KNeighborsRegressor', (9, 'chebyshev')) 0.5339805825242718 0.5059730712373647\n",
      "('KNeighborsRegressor', (10, 'euclidean')) 0.540453074433657 0.5207766990291263\n",
      "('KNeighborsRegressor', (10, 'manhattan')) 0.540453074433657 0.4903559870550161\n",
      "('KNeighborsRegressor', (10, 'chebyshev')) 0.5436893203883495 0.5234304207119741\n",
      "('KNeighborsRegressor', (11, 'euclidean')) 0.5436893203883495 0.5262510364010805\n",
      "('KNeighborsRegressor', (11, 'manhattan')) 0.5533980582524272 0.4979004520046003\n",
      "('KNeighborsRegressor', (11, 'chebyshev')) 0.5372168284789643 0.537537778491\n",
      "('KNeighborsRegressor', (12, 'euclidean')) 0.5598705501618123 0.538475368572456\n",
      "('KNeighborsRegressor', (12, 'manhattan')) 0.5469255663430421 0.5072815533980582\n",
      "('KNeighborsRegressor', (12, 'chebyshev')) 0.5436893203883495 0.5529261057173679\n",
      "('KNeighborsRegressor', (13, 'euclidean')) 0.5566343042071198 0.542348863484039\n",
      "('KNeighborsRegressor', (13, 'manhattan')) 0.5501618122977346 0.5160759081595527\n",
      "('KNeighborsRegressor', (13, 'chebyshev')) 0.5339805825242718 0.5630684973478103\n",
      "('KNeighborsRegressor', (14, 'euclidean')) 0.5469255663430421 0.5372003170200119\n",
      "('KNeighborsRegressor', (14, 'manhattan')) 0.5501618122977346 0.527805296876032\n",
      "('KNeighborsRegressor', (14, 'chebyshev')) 0.5436893203883495 0.5689353411267419\n",
      "('KNeighborsRegressor', (15, 'euclidean')) 0.5436893203883495 0.545329018338727\n",
      "('KNeighborsRegressor', (15, 'manhattan')) 0.5275080906148867 0.5400934915498022\n",
      "('KNeighborsRegressor', (15, 'chebyshev')) 0.5275080906148867 0.5757784969435454\n",
      "('KNeighborsRegressor', (16, 'euclidean')) 0.5533980582524272 0.5459142394822006\n",
      "('KNeighborsRegressor', (16, 'manhattan')) 0.5436893203883495 0.546217637540453\n",
      "('KNeighborsRegressor', (16, 'chebyshev')) 0.5372168284789643 0.5832448422330098\n",
      "('KNeighborsRegressor', (17, 'euclidean')) 0.5501618122977346 0.5543722914637014\n",
      "('KNeighborsRegressor', (17, 'manhattan')) 0.5436893203883495 0.5484485056158386\n",
      "('KNeighborsRegressor', (17, 'chebyshev')) 0.5242718446601942 0.587485022564137\n",
      "('KNeighborsRegressor', (18, 'euclidean')) 0.540453074433657 0.561718406648288\n",
      "('KNeighborsRegressor', (18, 'manhattan')) 0.5469255663430421 0.5484238283591035\n",
      "('KNeighborsRegressor', (18, 'chebyshev')) 0.5275080906148867 0.5925326621119502\n",
      "('KNeighborsRegressor', (19, 'euclidean')) 0.5501618122977346 0.5653479636751563\n",
      "('KNeighborsRegressor', (19, 'manhattan')) 0.5533980582524272 0.5560964239930434\n",
      "('KNeighborsRegressor', (19, 'chebyshev')) 0.5242718446601942 0.5985799962348386\n",
      "('RandomForestRegressor', 1) 0.6019417475728155 0.5286467936064484\n",
      "('RandomForestRegressor', 2) 0.6213592233009708 0.3500742774986897\n",
      "('RandomForestRegressor', 3) 0.6893203883495146 0.29952486427748415\n",
      "('RandomForestRegressor', 4) 0.6925566343042071 0.2847307741662525\n",
      "('RandomForestRegressor', 5) 0.6925566343042071 0.2771987798572414\n",
      "('RandomForestRegressor', 6) 0.6990291262135923 0.2746425436598604\n",
      "('RandomForestRegressor', 7) 0.7055016181229773 0.2746254898493601\n",
      "('RandomForestRegressor', 8) 0.6990291262135923 0.2718117790791438\n",
      "('RandomForestRegressor', 9) 0.7022653721682848 0.27363869135004204\n",
      "('RandomForestRegressor', 10) 0.7087378640776699 0.2720768397030292\n",
      "('RandomForestRegressor', 11) 0.7119741100323624 0.270947944547362\n",
      "('RandomForestRegressor', 12) 0.7184466019417476 0.26693832083956\n",
      "('RandomForestRegressor', 13) 0.7087378640776699 0.26720387953207075\n",
      "('RandomForestRegressor', 14) 0.7119741100323624 0.2745741218226105\n",
      "('RandomForestRegressor', 15) 0.7152103559870551 0.27360598924046553\n",
      "('RandomForestRegressor', 16) 0.7087378640776699 0.26609003854388097\n",
      "('RandomForestRegressor', 17) 0.6957928802588996 0.2661554183070618\n",
      "('RandomForestRegressor', 18) 0.6990291262135923 0.2674015132810467\n",
      "('RandomForestRegressor', 19) 0.6925566343042071 0.2670827881482454\n",
      "('XGBRegressor', 1) 0.6601941747572816 0.28259571078543244\n",
      "('XGBRegressor', 2) 0.686084142394822 0.2643110258098708\n",
      "('XGBRegressor', 3) 0.6990291262135923 0.268763325999721\n",
      "('XGBRegressor', 4) 0.6731391585760518 0.26992427580419603\n",
      "('XGBRegressor', 5) 0.6957928802588996 0.2864383005043149\n",
      "('XGBRegressor', 6) 0.6699029126213593 0.30595486372686653\n",
      "('XGBRegressor', 7) 0.6957928802588996 0.28770181104263\n",
      "('XGBRegressor', 8) 0.6828478964401294 0.31306722057401754\n",
      "('XGBRegressor', 9) 0.6407766990291263 0.31698985249756056\n",
      "('XGBRegressor', 10) 0.6634304207119741 0.33567387874314886\n",
      "('XGBRegressor', 11) 0.6537216828478964 0.32618960439300887\n",
      "('XGBRegressor', 12) 0.6699029126213593 0.3274203936933496\n",
      "('XGBRegressor', 13) 0.6407766990291263 0.3550234493681324\n",
      "('XGBRegressor', 14) 0.6504854368932039 0.33527659229336754\n",
      "('XGBRegressor', 15) 0.6472491909385113 0.33830683153225155\n",
      "('XGBRegressor', 16) 0.6504854368932039 0.36015630222924405\n",
      "('XGBRegressor', 17) 0.6375404530744336 0.35801926431431524\n",
      "('XGBRegressor', 18) 0.6310679611650486 0.36270839392471227\n",
      "('XGBRegressor', 19) 0.6310679611650486 0.3699823710791693\n",
      "('SVR', 1) 0.5469255663430421 0.604055424575244\n",
      "('SVR', 2) 0.5631067961165048 0.5520421057245533\n",
      "('SVR', 3) 0.5760517799352751 0.5369233848135694\n",
      "('SVR', 4) 0.5792880258899676 0.5268227144253906\n",
      "('SVR', 5) 0.56957928802589 0.5172696934311241\n",
      "('SVR', 6) 0.5728155339805825 0.5111355423705973\n",
      "('SVR', 7) 0.5760517799352751 0.5063035124298098\n",
      "('SVR', 8) 0.5760517799352751 0.5022453851665428\n",
      "('SVR', 9) 0.56957928802589 0.49882045018372695\n",
      "('SVR', 10) 0.56957928802589 0.49667670901668676\n",
      "('SVR', 11) 0.5792880258899676 0.4939597442639462\n",
      "('SVR', 12) 0.5792880258899676 0.49130107933243344\n",
      "('SVR', 13) 0.5792880258899676 0.48895893141986746\n",
      "('SVR', 14) 0.5889967637540453 0.4881160025793828\n",
      "('SVR', 15) 0.5954692556634305 0.4869873652709803\n",
      "('SVR', 16) 0.5954692556634305 0.4873132012499995\n",
      "('SVR', 17) 0.5954692556634305 0.4873490796960947\n",
      "('SVR', 18) 0.6019417475728155 0.48693618235361763\n",
      "('SVR', 19) 0.598705501618123 0.4861573664278051\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "for key in results:\n",
    "    print(key, results[key]['accuracy'], results[key]['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy:  (('XGBClassifier', 5), 0.7313915857605178)\n",
      "Min MSE:  (('XGBRegressor', 2), 0.2643110258098708)\n"
     ]
    }
   ],
   "source": [
    "# Highest accuracy\n",
    "accuracies = []\n",
    "mses = []\n",
    "\n",
    "for key in results:\n",
    "    accuracies.append((key, results[key]['accuracy']))\n",
    "    mses.append((key, results[key]['mse']))\n",
    "    \n",
    "    \n",
    "print(\"Max Accuracy: \", max(accuracies, key=lambda x: x[1]))\n",
    "print(\"Min MSE: \", min(mses, key=lambda x: x[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
